# Capstone-3-Project-Proposal
Project Overview
This project aims to distinguish between AI-generated and human-written content using a dataset of over 1,300 text samples. Each sample includes engineered features like word count, sentence length, grammar errors, readability scores, and more.

We apply both traditional machine learning and advanced models like XGBoost to build a reliable classifier that can detect AI content based on writing patterns.

Objective
To build an accurate, interpretable model that can:

Predict whether a piece of text was written by a human or generated by AI

Identify the most important linguistic features influencing the prediction

Dataset
Source: Kaggle

File: ai_human_content_detection_dataset.csv

Rows: 1,367

Features: 17 (including label, text_content, and numerical NLP features)

Features Used
Word & character metrics: word_count, character_count, avg_word_length

Grammar & structure: grammar_errors, passive_voice_ratio, punctuation_ratio

Readability: flesch_reading_ease, gunning_fog_index

Lexical: lexical_diversity, burstiness, predictability_score

Sentiment: sentiment_score

Models Built
Logistic Regression (baseline)

Random Forest Classifier

XGBoost Classifier

Each model was evaluated using:

Accuracy

Precision

Recall

F1 Score

Confusion Matrix

Key Findings
The dataset was perfectly balanced (Human vs AI).

XGBoost achieved ~88% accuracy.

Most influential features: word_count, grammar_errors, predictability_score, and burstiness.
